{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "for j in xrange(60000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n",
    "    l2_delta = (y - l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += X.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(csv_path):\n",
    "\tdata = pd.read_csv(csv_path)\n",
    "\tX = data.iloc[:, 0:2]\n",
    "\tY = data.iloc[:, 2]\n",
    "\treturn X, Y\n",
    "\n",
    "data_path = '/Users/aadi/Documents/ass4.csv'\n",
    "np.random.seed(1149)\n",
    "X, Y = get_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(csv_path):\n",
    "\tdata = pd.read_csv(csv_path)\n",
    "\tX = data.iloc[:, 0:2]\n",
    "\tY = data.iloc[:, 2]\n",
    "\treturn X, Y\n",
    "\n",
    "def activation(x, useDerivative=False):\n",
    "\tif(useDerivative==True):\n",
    "\t\tactivated = x*(1-x)\n",
    "\telse:\n",
    "\t\tactivated = 1/(1+np.exp(-x))\n",
    "\treturn activated\n",
    "\n",
    "def train_network(X, Y, weights, iterations):\n",
    "\tl0 = X\n",
    "\tfor i in range(0, iterations):\n",
    "\t\tl1 = activation(np.dot(l0, weights))\n",
    "\t\tl1 = l1.flatten()\n",
    "\t\tl1_error = np.asarray(Y) - np.asarray(l1)\n",
    "\t\tl1_delta = l1_error*activation(l1, True)\t\n",
    "\t\t#weights += \n",
    "\t\tf = np.dot(l0.T, l1_delta)\n",
    "\t\tweights = weights.T + f\n",
    "\t\tweights = weights.T\n",
    "        print(weights)\n",
    "\treturn l1\n",
    "\n",
    "def main():\n",
    "\tdata_path = '/Users/aadi/Documents/ass4.csv'\n",
    "\tnp.random.seed(1149)\n",
    "\tX, Y = get_data(data_path)\n",
    "\t\n",
    "\tw0 = 2*np.random.random((2, 1)) - 1\n",
    "\tw0 = np.asarray(w0)\n",
    "\tX = np.asarray(X)\n",
    "\tY = np.asarray(Y)\n",
    "\t#print(X.T.shape, w0.shape)\n",
    "\tresults = train_network(X, Y, w0, 20000)\n",
    "\tprint(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    X = data.iloc[:, 0:2]\n",
    "    Y = data.iloc[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "\n",
    "#X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "                \n",
    "#y = np.array([[0],[1],[1],[0]])\n",
    "data_path = '/Users/aadi/Documents/ass4.csv'\n",
    "X, y = get_data(data_path)\n",
    "y = np.asarray(y)\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((2, 100)) - 1\n",
    "syn1 = 2*np.random.random((100, 1)) - 1\n",
    "\n",
    "#print(type(y))\n",
    "\n",
    "for j in range(6):\n",
    "    print(\"y\", y.shape)\n",
    "    y = np.reshape(y, (100,1))\n",
    "\t# Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "    #l2 = l2.flatten()\n",
    "    print(\"l2\", l2.shape)\n",
    "    l2 = np.reshape(l2, (100,1))\n",
    "    #print(type(l2))\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = np.asarray(y) - np.asarray(l2)\n",
    "    l2_error = np.reshape(l2_error, (100,1))\n",
    "    print(\"l2 error\", l2_error.shape)\n",
    "\n",
    "    if (j% 1000) == 0:\n",
    "        print(\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "    print(\"l2 delta\", l2_delta.shape)\n",
    "    print(\"syn1\", syn1.T.shape)\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = np.dot(l2_delta,syn1.T)#.dot(syn1.T) #.T\n",
    "    print(\"l1 error\", l1_error.shape)\n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    print(\"l1_delta\", l1_delta.shape)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    print(\"syn1\", syn1.shape)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    print(\"syn0\", syn0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    X = data.iloc[:, 0:2]\n",
    "    Y = data.iloc[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "                \n",
    "#y = np.array([[0],[1],[1],[0]])\n",
    "data_path = '/Users/aadi/Documents/ass4.csv'\n",
    "X, y = get_data(data_path)\n",
    "y = np.asarray(y)\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((2, 100)) - 1\n",
    "syn1 = 2*np.random.random((100, 1)) - 1\n",
    "\n",
    "#print(type(y))\n",
    "\n",
    "for j in range(3000):\n",
    "    #print(\"y\", y.shape)\n",
    "    y = np.reshape(y, (100,1))\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "    #l2 = l2.flatten()\n",
    "    #print(\"l2\", l2.shape)\n",
    "    l2 = np.reshape(l2, (100,1))\n",
    "    #print(type(l2))\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    l2_error = np.reshape(l2_error, (100,1))\n",
    "    #print(\"l2 error\", l2_error.shape)\n",
    "\n",
    "    if (j% 1000) == 0:\n",
    "        print(\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "    #print(\"l2 delta\", l2_delta.shape)\n",
    "    #print(\"syn1\", syn1.T.shape)\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = np.dot(l2_delta,syn1.T)#.dot(syn1.T) #.T\n",
    "    #print(\"l1 error\", l1_error.shape)\n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    #print(\"l1_delta\", l1_delta.shape)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    #print(\"syn1\", syn1.shape)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    #print(\"syn0\", syn0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 82.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "##theano stuff\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as nnet\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    X = data.iloc[:, 0:2]\n",
    "    Y = data.iloc[:, 2]\n",
    "    return X, Y\n",
    "\n",
    "def layer(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) #theta1: 3x3 * x: 3x1 = 3x1 ;;; theta2: 1x4 * 4x1\n",
    "    h = m#nnet.relu(m)\n",
    "    return h\n",
    "\n",
    "def grad_desc(cost, theta):\n",
    "    alpha = 0.000001 #learning rate\n",
    "    return theta - (alpha * T.grad(cost, wrt=theta))\n",
    "    \n",
    "x = T.dvector()\n",
    "y = T.dscalar()\n",
    "\n",
    "theta1 = theano.shared(np.array(np.random.rand(3,3), dtype=theano.config.floatX)) # randomly initialize\n",
    "theta2 = theano.shared(np.array(np.random.rand(4,1), dtype=theano.config.floatX))\n",
    "\n",
    "hid1 = layer(x, theta1) #hidden layer\n",
    "out1 = T.sum(layer(hid1, theta2)) #output layer\n",
    "fc = abs((out1 - y))**2 #cost expression\n",
    "cost = theano.function(inputs=[x, y], outputs=fc, updates=[\n",
    "        (theta1, grad_desc(fc, theta1)),\n",
    "        (theta2, grad_desc(fc, theta2))])\n",
    "run_forward = theano.function(inputs=[x], outputs=out1)\n",
    "\n",
    "data_path = 'C:/Users/Aadi/Documents/ass4df1.csv'\n",
    "inputs, exp_y = get_data(data_path)\n",
    "inputs = np.asarray(inputs).reshape(200000,2)\n",
    "exp_y = np.asarray(exp_y).reshape(200000,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.388145259480586)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "cost(inputs[k], exp_y[k][0])\n",
    "#inputs[0]\n",
    "#exp_y[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 23.959478845684718\n",
      "Cost: 46.278792144913034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-83d6d456b2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcur_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#call our Theano-compiled cost function, it will auto update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[1;31m#if i % 100 == 0: #only print the cost every 500 epochs/iterations (to save space)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cost: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcur_cost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aadi\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#inputs = np.array([[0,1],[1,0],[1,1],[0,0]]).reshape(4,2) #training data X\n",
    "#exp_y = np.array([1, 1, 0, 0]) #training data Y\n",
    "cur_cost = 0\n",
    "for i in range(10000):\n",
    "    for k in range(len(inputs)):\n",
    "        cur_cost = cost(inputs[k], exp_y[k][0]) #call our Theano-compiled cost function, it will auto update weights\n",
    "    #if i % 100 == 0: #only print the cost every 500 epochs/iterations (to save space)\n",
    "    print('Cost: %s' % (cur_cost,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_forward([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = inputs\n",
    "y_train = exp_y\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=2))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "200000/200000 [==============================] - 11s - loss: 21.2587    \n",
      "Epoch 2/200\n",
      "200000/200000 [==============================] - 11s - loss: 11.9987    \n",
      "Epoch 3/200\n",
      "200000/200000 [==============================] - 11s - loss: 8.1330    \n",
      "Epoch 4/200\n",
      "200000/200000 [==============================] - 11s - loss: 6.1437    \n",
      "Epoch 5/200\n",
      "200000/200000 [==============================] - 11s - loss: 4.9930    \n",
      "Epoch 6/200\n",
      "200000/200000 [==============================] - 11s - loss: 4.2854    \n",
      "Epoch 7/200\n",
      "200000/200000 [==============================] - 11s - loss: 3.8295    \n",
      "Epoch 8/200\n",
      "200000/200000 [==============================] - 11s - loss: 3.5233    \n",
      "Epoch 9/200\n",
      "200000/200000 [==============================] - 11s - loss: 3.3095    \n",
      "Epoch 10/200\n",
      "200000/200000 [==============================] - 11s - loss: 3.1541    \n",
      "Epoch 11/200\n",
      "200000/200000 [==============================] - 11s - loss: 3.0364    \n",
      "Epoch 12/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.9434    \n",
      "Epoch 13/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.8668    \n",
      "Epoch 14/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.8008    \n",
      "Epoch 15/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.7418    \n",
      "Epoch 16/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.6872    \n",
      "Epoch 17/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.6358    \n",
      "Epoch 18/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.5875    \n",
      "Epoch 19/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.5420    \n",
      "Epoch 20/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.4995    \n",
      "Epoch 21/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.4596    \n",
      "Epoch 22/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.4222    \n",
      "Epoch 23/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.3870    \n",
      "Epoch 24/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.3538    \n",
      "Epoch 25/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.3223    \n",
      "Epoch 26/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.2924    \n",
      "Epoch 27/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.2640    \n",
      "Epoch 28/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.2370    \n",
      "Epoch 29/200\n",
      "200000/200000 [==============================] - 12s - loss: 2.2111    \n",
      "Epoch 30/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.1864    \n",
      "Epoch 31/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.1628    \n",
      "Epoch 32/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.1401    \n",
      "Epoch 33/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.1184    \n",
      "Epoch 34/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0975    \n",
      "Epoch 35/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0775    \n",
      "Epoch 36/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0581    \n",
      "Epoch 37/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0395    \n",
      "Epoch 38/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0216    \n",
      "Epoch 39/200\n",
      "200000/200000 [==============================] - 11s - loss: 2.0043    \n",
      "Epoch 40/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9877    \n",
      "Epoch 41/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9716    \n",
      "Epoch 42/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9561    \n",
      "Epoch 43/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9411    \n",
      "Epoch 44/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9266    \n",
      "Epoch 45/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.9125    \n",
      "Epoch 46/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8990    \n",
      "Epoch 47/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8858    \n",
      "Epoch 48/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8731    \n",
      "Epoch 49/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.8608    \n",
      "Epoch 50/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8488    \n",
      "Epoch 51/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8372    \n",
      "Epoch 52/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8260    \n",
      "Epoch 53/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8150    \n",
      "Epoch 54/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.8045    \n",
      "Epoch 55/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7942    \n",
      "Epoch 56/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7841    \n",
      "Epoch 57/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7744    \n",
      "Epoch 58/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7649    \n",
      "Epoch 59/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7557    \n",
      "Epoch 60/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7467    \n",
      "Epoch 61/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7380    \n",
      "Epoch 62/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7294    \n",
      "Epoch 63/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7211    \n",
      "Epoch 64/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7129    \n",
      "Epoch 65/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.7050    \n",
      "Epoch 66/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6972    \n",
      "Epoch 67/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6896    \n",
      "Epoch 68/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6821    \n",
      "Epoch 69/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6748    \n",
      "Epoch 70/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6677    \n",
      "Epoch 71/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6607    \n",
      "Epoch 72/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6537    \n",
      "Epoch 73/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6469    \n",
      "Epoch 74/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6402    \n",
      "Epoch 75/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6335    \n",
      "Epoch 76/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6270    \n",
      "Epoch 77/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6205    \n",
      "Epoch 78/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6141    \n",
      "Epoch 79/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6076    \n",
      "Epoch 80/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.6012    \n",
      "Epoch 81/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5948    \n",
      "Epoch 82/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5884    \n",
      "Epoch 83/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5820    \n",
      "Epoch 84/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5755    \n",
      "Epoch 85/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.5688    \n",
      "Epoch 86/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5620    \n",
      "Epoch 87/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5551    \n",
      "Epoch 88/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5479    \n",
      "Epoch 89/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5404    \n",
      "Epoch 90/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.5326    \n",
      "Epoch 91/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5242    \n",
      "Epoch 92/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5154    \n",
      "Epoch 93/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.5058    \n",
      "Epoch 94/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.4955    \n",
      "Epoch 95/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.4841    \n",
      "Epoch 96/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.4716    \n",
      "Epoch 97/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.4578    \n",
      "Epoch 98/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.4426    \n",
      "Epoch 99/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.4256    \n",
      "Epoch 100/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.4069    \n",
      "Epoch 101/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.3863    \n",
      "Epoch 102/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.3635    \n",
      "Epoch 103/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.3386    \n",
      "Epoch 104/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.3114    \n",
      "Epoch 105/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.2818    \n",
      "Epoch 106/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.2497    \n",
      "Epoch 107/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.2151    \n",
      "Epoch 108/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.1779    \n",
      "Epoch 109/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.1383    \n",
      "Epoch 110/200\n",
      "200000/200000 [==============================] - 12s - loss: 1.0965    \n",
      "Epoch 111/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.0528    \n",
      "Epoch 112/200\n",
      "200000/200000 [==============================] - 11s - loss: 1.0079    \n",
      "Epoch 113/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.9625    \n",
      "Epoch 114/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.9175    \n",
      "Epoch 115/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.8736    \n",
      "Epoch 116/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.8316    \n",
      "Epoch 117/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.7919    \n",
      "Epoch 118/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.7547    \n",
      "Epoch 119/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.7203    \n",
      "Epoch 120/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.6883    \n",
      "Epoch 121/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.6588    \n",
      "Epoch 122/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.6313    \n",
      "Epoch 123/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.6058    \n",
      "Epoch 124/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.5819    \n",
      "Epoch 125/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.5596    \n",
      "Epoch 126/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.5386    \n",
      "Epoch 127/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.5188    \n",
      "Epoch 128/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.5001    \n",
      "Epoch 129/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4824    \n",
      "Epoch 130/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4656    \n",
      "Epoch 131/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4497    \n",
      "Epoch 132/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4346    \n",
      "Epoch 133/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4202    \n",
      "Epoch 134/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.4065    \n",
      "Epoch 135/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3935    \n",
      "Epoch 136/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3811    \n",
      "Epoch 137/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3692    \n",
      "Epoch 138/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3580    \n",
      "Epoch 139/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3472    \n",
      "Epoch 140/200\n",
      "200000/200000 [==============================] - 11s - loss: 0.3370    \n",
      "Epoch 141/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.3272    \n",
      "Epoch 142/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.3178    \n",
      "Epoch 143/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.3089    \n",
      "Epoch 144/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.3004    \n",
      "Epoch 145/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2923    \n",
      "Epoch 146/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2846    \n",
      "Epoch 147/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2772    \n",
      "Epoch 148/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2701    \n",
      "Epoch 149/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2633    \n",
      "Epoch 150/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2569    \n",
      "Epoch 151/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2507    \n",
      "Epoch 152/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2448    \n",
      "Epoch 153/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2391    \n",
      "Epoch 154/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2337    \n",
      "Epoch 155/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2285    \n",
      "Epoch 156/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2236    \n",
      "Epoch 157/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2188    \n",
      "Epoch 158/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2143    \n",
      "Epoch 159/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2099    \n",
      "Epoch 160/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2057    \n",
      "Epoch 161/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.2017    \n",
      "Epoch 162/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1979    \n",
      "Epoch 163/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1942    \n",
      "Epoch 164/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1906    \n",
      "Epoch 165/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1873    \n",
      "Epoch 166/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1840    \n",
      "Epoch 167/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1809    \n",
      "Epoch 168/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1779    \n",
      "Epoch 169/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1750    \n",
      "Epoch 170/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1722    \n",
      "Epoch 171/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1695    \n",
      "Epoch 172/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1670    \n",
      "Epoch 173/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1645    \n",
      "Epoch 174/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1621    \n",
      "Epoch 175/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1598    \n",
      "Epoch 176/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1576    \n",
      "Epoch 177/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1555    \n",
      "Epoch 178/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1534    \n",
      "Epoch 179/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1515    \n",
      "Epoch 180/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1496    \n",
      "Epoch 181/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1477    \n",
      "Epoch 182/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1459    \n",
      "Epoch 183/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1442    \n",
      "Epoch 184/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1426    \n",
      "Epoch 185/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1409    \n",
      "Epoch 186/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1394    \n",
      "Epoch 187/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1379    \n",
      "Epoch 188/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1364    \n",
      "Epoch 189/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1350    \n",
      "Epoch 190/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1336    \n",
      "Epoch 191/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1323    \n",
      "Epoch 192/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1310    \n",
      "Epoch 193/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1298    \n",
      "Epoch 194/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1285    \n",
      "Epoch 195/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1274    \n",
      "Epoch 196/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1262    \n",
      "Epoch 197/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1251    \n",
      "Epoch 198/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1240    \n",
      "Epoch 199/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1230    \n",
      "Epoch 200/200\n",
      "200000/200000 [==============================] - 10s - loss: 0.1219    \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(0.5)\n",
    "model.fit(X_train, y_train, nb_epoch=200, batch_size=16)\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199840/200000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12140939481927664"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(X_train, y_train, batch_size=16)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12140939485939846"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
